{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./FIDs-features/\"\n",
    "\n",
    "def process(df, rType):\n",
    "    df['FID'] = df['p1'].str[:5]\n",
    "    df['p1'] = path+df['p1']\n",
    "    df['p2'] = path+df['p2']\n",
    "    df['p1'] = df['p1'].str.replace('.jpg', '.pkl')\n",
    "    df['p2'] = df['p2'].str.replace('.jpg', '.pkl')\n",
    "    \n",
    "    sample1 = df.sample(n=df.shape[0], replace=True)[['p1','FID']].reset_index().drop('index',axis=1)\n",
    "    sample2 = df.sample(n=df.shape[0], replace=True)[['p2','FID']].reset_index().drop('index',axis=1)\n",
    "    sample2.columns = ['p2','FID2']\n",
    "    unrelated = pd.concat([sample1,sample2],axis=1)\n",
    "    unrelated = unrelated[unrelated['FID'] != unrelated['FID2']][['p1','p2']]\n",
    "    \n",
    "    ###############################################################\n",
    "    # Related file\n",
    "    p1 = df['p1'].values.tolist()\n",
    "    p2 = df['p2'].values.tolist()\n",
    "    F = pd.read_pickle(p1[0]).reshape(1,512)\n",
    "    C = pd.read_pickle(p2[0]).reshape(1,512)\n",
    "    related_fs_set = np.append(F,C,axis=1)\n",
    "\n",
    "    for r1, r2 in zip(p1[1:],p2[1:]):\n",
    "        F = pd.read_pickle(r1).reshape(1,512)\n",
    "        C = pd.read_pickle(r2).reshape(1,512)\n",
    "        temp = np.append(F,C,axis=1)\n",
    "        related_fs_set = np.append(related_fs_set, temp, axis=0)\n",
    "        \n",
    "    pd.DataFrame(related_fs_set).to_pickle(\"Related_{}_1024.pkl\".format(rType))\n",
    "        \n",
    "    ###############################################################\n",
    "    # UnRelated file\n",
    "    p1 = unrelated['p1'].values.tolist()\n",
    "    p2 = unrelated['p2'].values.tolist()\n",
    "    F = pd.read_pickle(p1[0]).reshape(1,512)\n",
    "    C = pd.read_pickle(p2[0]).reshape(1,512)\n",
    "    unrelated_fs_set = np.append(F,C,axis=1)\n",
    "\n",
    "    for r1, r2 in zip(p1[1:],p2[1:]):\n",
    "        F = pd.read_pickle(r1).reshape(1,512)\n",
    "        C = pd.read_pickle(r2).reshape(1,512)\n",
    "        temp = np.append(F,C,axis=1)\n",
    "        unrelated_fs_set = np.append(unrelated_fs_set, temp, axis=0)\n",
    "        \n",
    "    pd.DataFrame(unrelated_fs_set).to_pickle(\"Unrelated_{}_1024.pkl\".format(rType))\n",
    "  \n",
    "# Example \n",
    "process(pd.read_pickle(\"./lists/pairs/pickles/Direct/fs-faces.pkl\"), \"fs\")\n",
    "process(pd.read_pickle(\"./lists/pairs/pickles/Direct/fd-faces.pkl\"), \"fd\")\n",
    "process(pd.read_pickle(\"./lists/pairs/pickles/Direct/ms-faces.pkl\"), \"ms\")\n",
    "process(pd.read_pickle(\"./lists/pairs/pickles/Direct/md-faces.pkl\"), \"md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_pickle(\"./Related_fs_1024.pkl\")\n",
    "p1['class'] = 1\n",
    "p2 = pd.read_pickle(\"./Unrelated_fs_1024.pkl\")\n",
    "p2['label'] = 0\n",
    "df_fs = pd.concat([p1, p2])\n",
    "df_fs['label'] = 'fs'\n",
    "#################################################################\n",
    "\n",
    "p1 = pd.read_pickle(\"./Related_fd_1024.pkl\")\n",
    "p1['class'] = 1\n",
    "p2 = pd.read_pickle(\"./Unrelated_fd_1024.pkl\")\n",
    "p2['class'] = 0\n",
    "df_fd = pd.concat([p1, p2])\n",
    "df_fd['label'] = 'fd'\n",
    "#################################################################\n",
    "\n",
    "p1 = pd.read_pickle(\"./Related_ms_1024.pkl\")\n",
    "p1['class'] = 1\n",
    "p2 = pd.read_pickle(\"./Unrelated_ms_1024.pkl\")\n",
    "p2['class'] = 0\n",
    "df_ms = pd.concat([p1, p2])\n",
    "df_ms['label'] = 'ms'\n",
    "\n",
    "#################################################################\n",
    "p1 = pd.read_pickle(\"./Related_md_1024.pkl\")\n",
    "p1['class'] = 1\n",
    "p2 = pd.read_pickle(\"./Unrelated_md_1024.pkl\")\n",
    "p2['class'] = 0\n",
    "df_md = pd.concat([p1, p2])\n",
    "df_md['label'] = 'md'\n",
    "\n",
    "\n",
    "df = pd.concat([df_fs, df_fd, df_ms, df_md])\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Family DataSet\n",
    "X_train = df.drop(['label','class'],axis=1)[0:300000].values.astype(np.float64)\n",
    "X_valid = df.drop(['label','class'],axis=1)[300000:370000].values.astype(np.float64)\n",
    "X_test = df.drop(['label','class'],axis=1)[370000:].values.astype(np.float64)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = df['class'][0:300000].values.astype(np.int64)\n",
    "y_valid = df['class'][300000:370000].values.astype(np.int64)\n",
    "y_test = df['class'][370000:].values.astype(np.int64)\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).type(torch.FloatTensor), torch.from_numpy(y_train))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(X_valid).type(torch.FloatTensor), torch.from_numpy(y_valid))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).type(torch.FloatTensor), torch.from_numpy(y_test))\n",
    "\n",
    "loaders = {}\n",
    "loaders['train'] = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "loaders['valid'] = DataLoader(valid_dataset, batch_size=200)\n",
    "loaders['test'] = DataLoader(test_dataset, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## relation DataSet\n",
    "realation_dict = {'fs':0, 'fd':1, 'ms':2, 'md':3}\n",
    "y_train = df['label'].map(realation_dict)[0:300000].values.astype(np.int64)\n",
    "y_valid = df['label'].map(realation_dict)[300000:370000].values.astype(np.int64)\n",
    "y_test = df['label'].map(realation_dict)[370000:].values.astype(np.int64)\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).type(torch.FloatTensor), torch.from_numpy(y_train))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(X_valid).type(torch.FloatTensor), torch.from_numpy(y_valid))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).type(torch.FloatTensor), torch.from_numpy(y_test))\n",
    "\n",
    "relation_loaders = {}\n",
    "relation_loaders['train'] = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "relation_loaders['valid'] = DataLoader(valid_dataset, batch_size=200)\n",
    "relation_loaders['test'] = DataLoader(test_dataset, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel()\n",
    "if ['class'] = 1\n",
    "    return label\n",
    "else \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.logSoftMax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.logSoftMax(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.693051 \tValidation Loss: 0.690293\n",
      "Validation loss decreased (inf --> 0.690293).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.545161 \tValidation Loss: 0.511763\n",
      "Validation loss decreased (0.524733 --> 0.511763).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.417602 \tValidation Loss: 0.364236\n",
      "Validation loss decreased (0.378216 --> 0.364236).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.336811 \tValidation Loss: 0.282483\n",
      "Validation loss decreased (0.284864 --> 0.282483).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.288169 \tValidation Loss: 0.239538\n",
      "Validation loss decreased (0.244177 --> 0.239538).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.257086 \tValidation Loss: 0.211998\n",
      "Validation loss decreased (0.214534 --> 0.211998).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.229341 \tValidation Loss: 0.193628\n",
      "Validation loss decreased (0.195296 --> 0.193628).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.210941 \tValidation Loss: 0.177830\n",
      "Validation loss decreased (0.181825 --> 0.177830).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.195424 \tValidation Loss: 0.169405\n",
      "Validation loss decreased (0.170806 --> 0.169405).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.182550 \tValidation Loss: 0.160238\n",
      "Validation loss decreased (0.160844 --> 0.160238).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.169911 \tValidation Loss: 0.154522\n",
      "Epoch: 110 \tTraining Loss: 0.163460 \tValidation Loss: 0.148364\n",
      "Epoch: 120 \tTraining Loss: 0.153396 \tValidation Loss: 0.144397\n",
      "Epoch: 130 \tTraining Loss: 0.146639 \tValidation Loss: 0.138224\n",
      "Epoch: 140 \tTraining Loss: 0.141184 \tValidation Loss: 0.136388\n",
      "Epoch: 150 \tTraining Loss: 0.135173 \tValidation Loss: 0.132692\n",
      "Epoch: 160 \tTraining Loss: 0.131741 \tValidation Loss: 0.129894\n",
      "Validation loss decreased (0.130217 --> 0.129894).  Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 0.127064 \tValidation Loss: 0.127662\n",
      "Epoch: 180 \tTraining Loss: 0.123835 \tValidation Loss: 0.126851\n",
      "Epoch: 190 \tTraining Loss: 0.120307 \tValidation Loss: 0.125174\n"
     ]
    }
   ],
   "source": [
    "\"\"\"returns trained model\"\"\"\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf \n",
    "\n",
    "for epoch in range(200):\n",
    "    # initialize variables to monitor training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        optimizer.zero_grad()\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        ## find the loss and update the model parameters accordingly\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ## record the average training loss, using something like\n",
    "        train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        ## update the average validation loss\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "    if(epoch % 10 == 0):\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "    ## TODO: save the model if validation loss has decreased\n",
    "    if valid_loss < valid_loss_min:\n",
    "        if(epoch % 10 == 0):\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "        torch.save(model.state_dict(), \"checkpoint.cpt\")\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.124207\n",
      "\n",
      "\n",
      "Test Accuracy: 95% (65273/68579)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.cpt'))\n",
    "# monitor test loss and accuracy\n",
    "test_loss = 0.\n",
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "model.eval()\n",
    "for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "    # move to GPU\n",
    "    if use_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "    # convert output probabilities to predicted class\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    # compare predictions to true label\n",
    "    correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "    total += data.size(0)\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "    100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TypeModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 4)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.logSoftMax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.logSoftMax(self.output(x))\n",
    "        return x\n",
    "\n",
    "relation_model = TypeModel()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    relation_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_criterion = nn.NLLLoss()\n",
    "relation_optimizer = optim.SGD(relation_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.273120 \tValidation Loss: 0.941659\n",
      "Validation loss decreased (inf --> 0.941659).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.136390 \tValidation Loss: 0.055443\n",
      "Validation loss decreased (0.064613 --> 0.055443).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.062214 \tValidation Loss: 0.016665\n",
      "Validation loss decreased (0.018802 --> 0.016665).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"returns trained model\"\"\"\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf \n",
    "\n",
    "for epoch in range(30):\n",
    "    # initialize variables to monitor training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    relation_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(relation_loaders['train']):\n",
    "        relation_optimizer.zero_grad()\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        ## find the loss and update the model parameters accordingly\n",
    "        pred = relation_model(data)\n",
    "        loss = relation_criterion(pred, target)\n",
    "        loss.backward()\n",
    "        relation_optimizer.step()\n",
    "        ## record the average training loss, using something like\n",
    "        train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    relation_model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(relation_loaders['valid']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        ## update the average validation loss\n",
    "        pred = relation_model(data)\n",
    "        loss = relation_criterion(pred, target)\n",
    "        valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "    if(epoch % 10 == 0):\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "    ## TODO: save the model if validation loss has decreased\n",
    "    if valid_loss < valid_loss_min:\n",
    "        if(epoch % 10 == 0):\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "        torch.save(relation_model.state_dict(), \"relation_checkpoint.cpt\")\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.008401\n",
      "\n",
      "\n",
      "Test Accuracy: 99% (68403/68579)\n"
     ]
    }
   ],
   "source": [
    "relation_model.load_state_dict(torch.load('relation_checkpoint.cpt'))\n",
    "# monitor test loss and accuracy\n",
    "test_loss = 0.\n",
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "relation_model.eval()\n",
    "for batch_idx, (data, target) in enumerate(relation_loaders['test']):\n",
    "    # move to GPU\n",
    "    if use_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = relation_model(data)\n",
    "    # calculate the loss\n",
    "    loss = relation_criterion(output, target)\n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "    # convert output probabilities to predicted class\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    # compare predictions to true label\n",
    "    correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "    total += data.size(0)\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "    100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
